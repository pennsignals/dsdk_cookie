{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from os import chdir\n",
    "from typing import Any\n",
    "\n",
    "from cfgenvy import Parser, YamlMapping, yaml_dumps\n",
    "from dsdk import Mssql, Postgres\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "If the module is needed, install it once, and reload the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chdir(\"/tmp\")\n",
    "try:\n",
    "    from {{cookiecutter.name}} import Service\n",
    "\n",
    "    _ = Service\n",
    "except ImportError:\n",
    "    # !pip install -e \".[dev]\"\n",
    "    raise \"Module has been installed, please restart the kernel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Manage Configuration & Environment\n",
    "\n",
    "### 2.1. Files\n",
    "\n",
    "Because deserializing objects is less error prone than (re-)configuring previously existing python objects, use cfgenvy to load and dump yaml as configuration. Merge environment variable files into yaml configuration during deserialization, and keep your secrets separate and safe.\n",
    "\n",
    "Secrets as well as differences among deployment environments are placed in .env files: `./predict/secrets/`.\n",
    "\n",
    "Configurations are placed in .yaml files: `./predict/local/`.\n",
    "\n",
    "These directories have .gitignore protection from accidental inclusion in version control. See the `./predict/secrets/.gitignore` and `./predict/local/.gitignore` files for file names that *ARE* included in version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chdir(\"/tmp\")\n",
    "config_file = \"./predict/local/notebook.example.yaml\"\n",
    "env_file = \"./predict/secrets/notebook.example.env\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Service names are resolved to host ip addresses by docker DNS as listed in docker-compose.override.yaml, and later by consul DNS in production. Use service names when possible instead of ip addresses. Even names for external services external like clarity, and epic can be registered in consul DNS to keep ip addresses out of configuration files.\n",
    "\n",
    "Here the MSSQL_HOST and POSTGRES_HOST are service names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs_str = \"\"\"\n",
    "EPIC_COOKIE=cookie\n",
    "MSSQL_DATABASE=clarity\n",
    "MSSQL_HOST=mssql\n",
    "MSSQL_PASSWORD=password\n",
    "MSSQL_PORT=1433\n",
    "MSSQL_USERNAME=username\n",
    "POSTGRES_DATABASE=test\n",
    "POSTGRES_HOST=postgres\n",
    "POSTGRES_PASSWORD=password\n",
    "POSTGRES_PORT=5432\n",
    "POSTGRES_SCHEMA=test\n",
    "POSTGRES_USERNAME=postgres\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(env_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.write(envs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs_str = \"\"\"\n",
    "elixhauser:\n",
    "  key1: val1\n",
    "  key2: val2\n",
    "  key3: val3\n",
    "mssql: !mssql\n",
    "  database: ${MSSQL_DATABASE}\n",
    "  host: ${MSSQL_HOST}\n",
    "  password: ${MSSQL_PASSWORD}\n",
    "  port: ${MSSQL_PORT}\n",
    "  schema: test\n",
    "  sql: !asset\n",
    "    path: ./predict/sql/mssql\n",
    "    ext: .sql\n",
    "  username: ${MSSQL_USERNAME}\n",
    "postgres: !postgres\n",
    "  database: ${POSTGRES_DATABASE}\n",
    "  host: ${POSTGRES_HOST}\n",
    "  password: ${POSTGRES_PASSWORD}\n",
    "  port: ${POSTGRES_PORT}\n",
    "  schema: test\n",
    "  sql: !asset\n",
    "    path: ./predict/sql/postgres\n",
    "    ext: .sql\n",
    "  username: ${POSTGRES_USERNAME}\n",
    "stages:\n",
    "- first\n",
    "- second\n",
    "- third\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.write(cfgs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Register classes as yaml types so they may be deserialized as instaces of python classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mssql.as_yaml_type()\n",
    "Postgres.as_yaml_type()\n",
    "\n",
    "cfg = Parser.load(\n",
    "    config_file=config_file,\n",
    "    env_file=env_file,\n",
    ")\n",
    "\n",
    "print(f\"type(cfg): {type(cfg)}\")\n",
    "print(f\"type(cfg['elixhauser']: {type(cfg['elixhauser'])}\")\n",
    "print(f\"type(cfg['postgres']: {type(cfg['postgres'])}\")\n",
    "print(f\"type(cfg['postgres'].sql: {type(cfg['postgres'].sql)}\")\n",
    "print(f\"type(cfg['stages']): {type(cfg['stages'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Create and register a class to provide better validation for confguration and by ensuring that the configuration file is not mismatched, use explicit yaml `!<type>` and a clss. Unlike a python dictionary, unexpected or missing keywords will raise early exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.write(\"!cfg\" + cfgs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cfg(YamlMapping):\n",
    "    YAML = \"!cfg\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        elixhauser: dict[str, str],\n",
    "        mssql: Mssql,\n",
    "        postgres: Postgres,\n",
    "        stages: list,\n",
    "    ):\n",
    "        self.elixhauser = elixhauser\n",
    "        self.mssql = mssql\n",
    "        self.postgres = postgres\n",
    "        self.stages = stages\n",
    "\n",
    "    def as_yaml(self) -> dict[str, Any]:\n",
    "        \"\"\"As yaml.\"\"\"\n",
    "        return {\n",
    "            \"elixhauser\": self.elixhauser,\n",
    "            \"mssql\": self.mssql,\n",
    "            \"postgres\": self.postgres,\n",
    "            \"stages\": self.stages,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cfg.as_yaml_type()\n",
    "\n",
    "cfg = Parser.load(\n",
    "    config_file=config_file,\n",
    "    env_file=env_file,\n",
    ")\n",
    "\n",
    "print(f\"type(cfg): {type(cfg)}\")\n",
    "print(f\"type(cfg.elixhauser): {type(cfg.elixhauser)}\")\n",
    "print(f\"type(cfg.postgres): {type(cfg.postgres)}\")\n",
    "print(f\"type(cfg.postgres.sql): {type(cfg.postgres.sql)}\")\n",
    "print(f\"type(cfg.stages): {type(cfg.stages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres = cfg.postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Debug the final merged configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yaml_dumps(postgres))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 3. Check Database Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with postgres.rollback() as cursor:\n",
    "    cursor.execute(\"\"\"select 'Very database, much wow!' as doge\"\"\")\n",
    "    rows = cursor.fetchall()\n",
    "    print(rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 4. Manage SQL & Other Text Assets\n",
    "\n",
    "Assets loads text files from disk. Unlike SQL embedded in python strings, SQL syntax highlighting may be available in text editor. The python placeholders expected by psycopg2 and pymssql will still be marked as errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(postgres.sql.predictions.gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# 5. Rethink SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\n",
    "    \"cohort\": (\"00001\", \"00002\", \"00003\"),\n",
    "    \"conditions\": (\"sleepy\", \"happy\", \"grumpy\"),\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    \"dry_run\": 0,\n",
    "    \"cohort_begin\": \"2021-05-05\",\n",
    "    \"cohort_end\": \"2021-05-06\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 5.1. Prefer `with` over `in (?, ...)`:\n",
    "\n",
    "Avoid `in` for more than a few elements:\n",
    "\n",
    "`select * from patients where id in ('00001', '00002', '00003', ...);`\n",
    "\n",
    "Unfortunately, the execution plan renders `in` similar to multiple `or`:\n",
    "\n",
    "`select * from patients where id = '00001' or id = '00002' or id = '00003' ...;`\n",
    "\n",
    "The performance is terrible. The database has limits on the number of elements that may be included using `in (?, ...)`. Fundamentally, the database does not treat `in` like a table with a single column, in part because the column data type is not known. Client languages like python typically only have data types that approximately match the database's data types. For example the pymssql driver passes all python strings to mssql as `nvarchar` literals ('n' is not a typo). Each element is coherced to the most permissive data type during comparison. This implicit, permissive casting and cohersion prevents indices from being used.\n",
    "\n",
    "Use `with` instead and `cast` the column to the appropriate data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 5.1.1. Example:\n",
    "\n",
    "An easy example in templated sql for python and dsdk looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5_1_1 = \"\"\"\n",
    "with cohort as (\n",
    "    -- data type is on the cohort.id column, not just this first row\n",
    "    select cast(null as varchar(8)) as id\n",
    "    {cohort}\n",
    ")\n",
    "select\n",
    "    id\n",
    "from\n",
    "    cohort\n",
    "where\n",
    "    id is not null;\"\"\"\n",
    "\n",
    "with postgres.rollback() as cur:\n",
    "    df = postgres.df_from_query(\n",
    "        cur,\n",
    "        query_5_1_1,\n",
    "        keys=keys,\n",
    "        parameters=parameters,\n",
    "    )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 5.1.2. Example:\n",
    "\n",
    "A more useful example using dsdk looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5_1_2 = \"\"\"\n",
    "with args as (\n",
    "    select\n",
    "        cast(%(cohort_begin)s as timestamptz) as cohort_begin,\n",
    "        cast(%(cohort_end)s as timestamptz) as cohort_end\n",
    "), cohort as (\n",
    "    select cast(null as varchar(8)) as id\n",
    "    {cohort}\n",
    "), conditions as (\n",
    "    select cast(null as varchar(16)) as name\n",
    "    {conditions}\n",
    ")\n",
    "select\n",
    "    cohort_begin,\n",
    "    cohort_end,\n",
    "    id,\n",
    "    name\n",
    "from\n",
    "    args\n",
    "    join cohort\n",
    "        on id is not null\n",
    "    join conditions\n",
    "        on name is not null;\n",
    "\"\"\"\n",
    "\n",
    "with postgres.rollback() as cur:\n",
    "    df = postgres.df_from_query(\n",
    "        cur,\n",
    "        query_5_1_2,\n",
    "        keys=keys,\n",
    "        parameters=parameters,\n",
    "    )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### 5.1.3. Example\n",
    "\n",
    "Implementation of dsdk for df_from_query_by_keys uses `union all select` implementation. This formulation avoids item limits as well as comma counting of `insert (...) values (...), ...`. Unlike `in` and `insert (...) values (...), ...` it also results in perfectly valid sql even when the cohort or conditions lists are empty, because the empty lists render as code while retaining the column data type(s) using a \"null row\".\n",
    "\n",
    "Unwind the sequences and replace the placeholders in pgadmin, DBeaver, Data Grip, and Microsort Sql Server Management Studio to test and explain your queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5_1_3 = \"\"\"\n",
    "with args as (\n",
    "    select\n",
    "        cast('2021-05-05' as timestamptz) as cohort_begin,\n",
    "        cast('2021-05-06' as timestamptz) as cohort_end\n",
    "), cohort as (\n",
    "    select cast(null as varchar) as id\n",
    "    union all select '00001'\n",
    "    union all select '00002'\n",
    "    union all select '00003'\n",
    "), conditions as (\n",
    "    select cast(null as varchar) as name\n",
    "    union all select 'happy'\n",
    "    union all select 'sleepy'\n",
    "    union all select 'grumpy'\n",
    ")\n",
    "select\n",
    "    cohort_begin,\n",
    "    cohort_end,\n",
    "    id,\n",
    "    name\n",
    "from\n",
    "    args\n",
    "    join cohort\n",
    "        on id is not null\n",
    "    join conditions\n",
    "        on name is not null;\"\"\"\n",
    "\n",
    "with postgres.rollback() as cur:\n",
    "    cur.execute(query_5_1_3)\n",
    "    rows = cur.fetchall()\n",
    "    df = DataFrame(rows)\n",
    "    columns = (each[0] for each in cur.description)\n",
    "    df.columns = columns\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 5.2 Use dry run to fail early\n",
    "\n",
    "Make the database do more work for you. This includes validating some syntax and all permission on the service accounts BEFORE passing actual useful data to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5_2 = \"\"\"\n",
    "with vars as (\n",
    "    select\n",
    "        cast(coalesce(%(dry_run)s, 1) as int) as dry_run,\n",
    "        cast(%(cohort_begin)s as timestamptz) as cohort_begin,\n",
    "        cast(%(cohort_end)s as timestamptz) as cohort_end\n",
    "), cohort as (\n",
    "    select cast(null as varchar) as id\n",
    "    {cohort}\n",
    ")\n",
    "select\n",
    "    no_such_table.*\n",
    "from\n",
    "    vars as v\n",
    "    join cohort as c\n",
    "        on v.dry_run = 0\n",
    "        and c.id is not null;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres.dry_run_query(query_5_2, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Persistors can dry run all sql queries in an asset if all parameters are provided. All queries must be written to select, insert, update or delete no data when dry_run is 1, but must do by producing empty data sets for insert, update, and delete instead of exiting early.\n",
    "\n",
    "Typically, this means using a `with` clause to build a data set for insert, update or delete and performing a join on `dry_run = 0` that knocks out all rows from the data manipulation operators.\n",
    "\n",
    "More examples to come, and all queries in the postgres persistor asset must be revised for dry_run compatibility.\n",
    "\n",
    "More examples to come on when to add unused tables to aquire indices.\n",
    "\n",
    "More examples to come on sql performance profiling and explain.\n",
    "\n",
    "More examples on when using temp tables may be an advantage, and the impact on readability, maintainability, and testing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
