{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5907a0",
   "metadata": {},
   "source": [
    "# 1. Jupyterlab\n",
    "\n",
    "## 1.1. Setup\n",
    "\n",
    "1. Choose the `Python [conda env:.venv]` from the menu in the top right corner of this page.\n",
    "\n",
    "2. The `dsdk` and `cfgenvy` modules are already installed into the conda venv, along with any dependencies added to requirement.txt SO LONG AS jupyterlab has been started with `docker-compose up --build jupyterlab &`. The docker container image build for jupyterlab will cache build steps applying `./jupyterlab/environment.yaml` with from modules anaconda and conda forge, and will apply `./requirements.txt` last with modules from pypi, or specifically pulled from public git repositories --this is a way to install your own data science helper modules like dsdk--. Only the first run of the jupyterlab container image build will be slow so long as environment.yaml is not changed.\n",
    "\n",
    "2. If there is a predict python module developed with this project, it may also be installed into the conda env, but this step is optional and may not be useful: --see setup.py--:\n",
    "\n",
    "    a. Production vs development mode: `pip install ...` vs. `pip install -e ...`. Development mode builds symlinks between site-packages and the code in src. Reinstall is not required after changes, but there other are limitations.\n",
    "\n",
    "    b. Sparce vs with additional dependencies: `pip install .` vs. `pip install .[all]`. Additional dependencies include lint and test tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b225ea69-c7ce-4818-a709-759332937b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///tmp\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dsdk[psycopg2,pymssql]@ git+https://github.com/pennsignals/dsdk.git@1.4.7#egg=dsdk\n",
      "  Cloning https://github.com/pennsignals/dsdk.git (to revision 1.4.7) to ./pip-install-mfj6y7wx/dsdk_e65ce6a8e72145b58d714e9b238e1444\n",
      "  Running command git clone --filter=blob:none -q https://github.com/pennsignals/dsdk.git /tmp/pip-install-mfj6y7wx/dsdk_e65ce6a8e72145b58d714e9b238e1444\n",
      "  Resolved https://github.com/pennsignals/dsdk.git to commit d6513c8053a2017281aa89c712b9b41bb685ea61\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /root/.venv/lib/python3.9/site-packages (from example==1.2.0rc4.dev3) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /root/.venv/lib/python3.9/site-packages (from example==1.2.0rc4.dev3) (1.22.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /root/.venv/lib/python3.9/site-packages (from example==1.2.0rc4.dev3) (1.0.2)\n",
      "Requirement already satisfied: pip>=21.3.1 in /root/.venv/lib/python3.9/site-packages (from example==1.2.0rc4.dev3) (21.3.1)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /root/.venv/lib/python3.9/site-packages (from example==1.2.0rc4.dev3) (1.7.3)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /root/.venv/lib/python3.9/site-packages (from example==1.2.0rc4.dev3) (1.3.5)\n",
      "Collecting pre-commit\n",
      "  Downloading pre_commit-2.16.0-py2.py3-none-any.whl (191 kB)\n",
      "     |████████████████████████████████| 191 kB 1.8 MB/s            \n",
      "\u001b[?25hCollecting pep8-naming\n",
      "  Downloading pep8_naming-0.12.1-py2.py3-none-any.whl (8.9 kB)\n",
      "Collecting flake8-logging-format\n",
      "  Downloading flake8-logging-format-0.6.0.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytest\n",
      "  Downloading pytest-6.2.5-py3-none-any.whl (280 kB)\n",
      "     |████████████████████████████████| 280 kB 3.5 MB/s            \n",
      "\u001b[?25hCollecting pylint\n",
      "  Downloading pylint-2.12.2-py3-none-any.whl (414 kB)\n",
      "     |████████████████████████████████| 414 kB 2.8 MB/s            \n",
      "\u001b[?25hCollecting coverage[toml]\n",
      "  Downloading coverage-6.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (215 kB)\n",
      "     |████████████████████████████████| 215 kB 2.3 MB/s            \n",
      "\u001b[?25hCollecting flake8-comprehensions\n",
      "  Downloading flake8_comprehensions-3.8.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting flake8-sorted-keys\n",
      "  Downloading flake8_sorted_keys-0.2.0-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting types-pkg-resources\n",
      "  Downloading types_pkg_resources-0.1.3-py2.py3-none-any.whl (4.8 kB)\n",
      "Collecting pytest-cov\n",
      "  Downloading pytest_cov-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting types-python-dateutil\n",
      "  Downloading types_python_dateutil-2.8.6-py3-none-any.whl (7.6 kB)\n",
      "Collecting mypy\n",
      "  Downloading mypy-0.931-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.6 MB)\n",
      "     |████████████████████████████████| 16.6 MB 2.8 MB/s            \n",
      "\u001b[?25hCollecting flake8-docstrings\n",
      "  Downloading flake8_docstrings-1.6.0-py2.py3-none-any.whl (5.7 kB)\n",
      "Collecting flake8-commas\n",
      "  Downloading flake8_commas-2.1.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting astroid\n",
      "  Downloading astroid-2.9.3-py3-none-any.whl (254 kB)\n",
      "     |████████████████████████████████| 254 kB 2.2 MB/s            \n",
      "\u001b[?25hCollecting flake8-mutable\n",
      "  Downloading flake8-mutable-1.2.0.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: black in /root/.venv/lib/python3.9/site-packages (from example==1.2.0rc4.dev3) (21.12b0)\n",
      "Collecting flake8\n",
      "  Downloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
      "     |████████████████████████████████| 64 kB 2.4 MB/s            \n",
      "\u001b[?25hCollecting flake8-bugbear\n",
      "  Downloading flake8_bugbear-22.1.11-py3-none-any.whl (18 kB)\n",
      "Collecting types-pyyaml\n",
      "  Downloading types_PyYAML-6.0.3-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /root/.venv/lib/python3.9/site-packages (from pandas>=1.3.5->example==1.2.0rc4.dev3) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/.venv/lib/python3.9/site-packages (from python-dateutil->example==1.2.0rc4.dev3) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/.venv/lib/python3.9/site-packages (from scikit-learn>=1.0.2->example==1.2.0rc4.dev3) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /root/.venv/lib/python3.9/site-packages (from scikit-learn>=1.0.2->example==1.2.0rc4.dev3) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10 in /root/.venv/lib/python3.9/site-packages (from astroid->example==1.2.0rc4.dev3) (4.0.1)\n",
      "Requirement already satisfied: setuptools>=20.0 in /root/.venv/lib/python3.9/site-packages (from astroid->example==1.2.0rc4.dev3) (60.5.0)\n",
      "Collecting lazy-object-proxy>=1.4.0\n",
      "  Downloading lazy_object_proxy-1.7.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61 kB)\n",
      "     |████████████████████████████████| 61 kB 3.0 MB/s            \n",
      "\u001b[?25hCollecting wrapt<1.14,>=1.11\n",
      "  Downloading wrapt-1.13.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (81 kB)\n",
      "     |████████████████████████████████| 81 kB 2.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=2 in /root/.venv/lib/python3.9/site-packages (from black->example==1.2.0rc4.dev3) (2.3.0)\n",
      "Requirement already satisfied: pathspec<1,>=0.9.0 in /root/.venv/lib/python3.9/site-packages (from black->example==1.2.0rc4.dev3) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /root/.venv/lib/python3.9/site-packages (from black->example==1.2.0rc4.dev3) (0.4.3)\n",
      "Requirement already satisfied: click>=7.1.2 in /root/.venv/lib/python3.9/site-packages (from black->example==1.2.0rc4.dev3) (8.0.3)\n",
      "Requirement already satisfied: tomli<2.0.0,>=0.2.6 in /root/.venv/lib/python3.9/site-packages (from black->example==1.2.0rc4.dev3) (1.2.2)\n",
      "Collecting cfgenvy@ git+https://github.com/pennsignals/cfgenvy.git@1.3.1#egg=cfgenvy\n",
      "  Cloning https://github.com/pennsignals/cfgenvy.git (to revision 1.3.1) to ./pip-install-mfj6y7wx/cfgenvy_5f31c1be032a451088615a4e6f3bd5c7\n",
      "  Running command git clone --filter=blob:none -q https://github.com/pennsignals/cfgenvy.git /tmp/pip-install-mfj6y7wx/cfgenvy_5f31c1be032a451088615a4e6f3bd5c7\n",
      "  Resolved https://github.com/pennsignals/cfgenvy.git to commit 715bd44632af2d9f24737372cd9fc4014f35a538\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /root/.venv/lib/python3.9/site-packages (from dsdk[psycopg2,pymssql]@ git+https://github.com/pennsignals/dsdk.git@1.4.7#egg=dsdk->example==1.2.0rc4.dev3) (2.27.0)\n",
      "Requirement already satisfied: wheel>=0.37.1 in /root/.venv/lib/python3.9/site-packages (from dsdk[psycopg2,pymssql]@ git+https://github.com/pennsignals/dsdk.git@1.4.7#egg=dsdk->example==1.2.0rc4.dev3) (0.37.1)\n",
      "Requirement already satisfied: cython>=0.29.21 in /root/.venv/lib/python3.9/site-packages (from dsdk[psycopg2,pymssql]@ git+https://github.com/pennsignals/dsdk.git@1.4.7#egg=dsdk->example==1.2.0rc4.dev3) (0.29.26)\n",
      "Requirement already satisfied: pymssql>=2.2.3 in /root/.venv/lib/python3.9/site-packages (from dsdk[psycopg2,pymssql]@ git+https://github.com/pennsignals/dsdk.git@1.4.7#egg=dsdk->example==1.2.0rc4.dev3) (2.2.3)\n",
      "Requirement already satisfied: psycopg2-binary>=2.8.6 in /root/.venv/lib/python3.9/site-packages (from dsdk[psycopg2,pymssql]@ git+https://github.com/pennsignals/dsdk.git@1.4.7#egg=dsdk->example==1.2.0rc4.dev3) (2.9.3)\n",
      "Collecting pyflakes<2.5.0,>=2.4.0\n",
      "  Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
      "     |████████████████████████████████| 69 kB 2.2 MB/s            \n",
      "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting pycodestyle<2.9.0,>=2.8.0\n",
      "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 1.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /root/.venv/lib/python3.9/site-packages (from flake8-bugbear->example==1.2.0rc4.dev3) (21.4.0)\n",
      "Collecting pydocstyle>=2.1\n",
      "  Downloading pydocstyle-6.1.1-py3-none-any.whl (37 kB)\n",
      "Collecting flake8-polyfill<2,>=1.0.2\n",
      "  Downloading flake8_polyfill-1.0.2-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/.venv/lib/python3.9/site-packages (from pre-commit->example==1.2.0rc4.dev3) (6.0)\n",
      "Collecting cfgv>=2.0.0\n",
      "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting virtualenv>=20.0.8\n",
      "  Downloading virtualenv-20.13.0-py2.py3-none-any.whl (6.5 MB)\n",
      "     |████████████████████████████████| 6.5 MB 2.3 MB/s            \n",
      "\u001b[?25hCollecting identify>=1.0.0\n",
      "  Downloading identify-2.4.4-py2.py3-none-any.whl (98 kB)\n",
      "     |████████████████████████████████| 98 kB 2.4 MB/s            \n",
      "\u001b[?25hCollecting nodeenv>=0.11.1\n",
      "  Downloading nodeenv-1.6.0-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: isort<6,>=4.2.5 in /root/.venv/lib/python3.9/site-packages (from pylint->example==1.2.0rc4.dev3) (5.10.1)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "     |████████████████████████████████| 98 kB 2.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging in /root/.venv/lib/python3.9/site-packages (from pytest->example==1.2.0rc4.dev3) (21.3)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting snowballstemmer\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "     |████████████████████████████████| 93 kB 638 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /root/.venv/lib/python3.9/site-packages (from requests>=2.26.0->dsdk[psycopg2,pymssql]@ git+https://github.com/pennsignals/dsdk.git@1.4.7#egg=dsdk->example==1.2.0rc4.dev3) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /root/.venv/lib/python3.9/site-packages (from requests>=2.26.0->dsdk[psycopg2,pymssql]@ git+https://github.com/pennsignals/dsdk.git@1.4.7#egg=dsdk->example==1.2.0rc4.dev3) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/.venv/lib/python3.9/site-packages (from requests>=2.26.0->dsdk[psycopg2,pymssql]@ git+https://github.com/pennsignals/dsdk.git@1.4.7#egg=dsdk->example==1.2.0rc4.dev3) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/.venv/lib/python3.9/site-packages (from requests>=2.26.0->dsdk[psycopg2,pymssql]@ git+https://github.com/pennsignals/dsdk.git@1.4.7#egg=dsdk->example==1.2.0rc4.dev3) (2021.10.8)\n",
      "Collecting distlib<1,>=0.3.1\n",
      "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
      "     |████████████████████████████████| 461 kB 1.3 MB/s            \n",
      "\u001b[?25hCollecting filelock<4,>=3.2\n",
      "  Downloading filelock-3.4.2-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /root/.venv/lib/python3.9/site-packages (from packaging->pytest->example==1.2.0rc4.dev3) (3.0.6)\n",
      "Building wheels for collected packages: flake8-logging-format, flake8-mutable, dsdk\n",
      "  Building wheel for flake8-logging-format (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flake8-logging-format: filename=flake8_logging_format-0.6.0-py2.py3-none-any.whl size=5471 sha256=c516ca757c0f08122c778d3129d439becd59919909c51312e8cadab04a472252\n",
      "  Stored in directory: /root/.cache/pip/wheels/f3/57/0f/b903222af25203d5fdad588f6afd5d101a0f6b1802c99821e7\n",
      "  Building wheel for flake8-mutable (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flake8-mutable: filename=flake8_mutable-1.2.0-py3-none-any.whl size=3326 sha256=9d8c85e01833e7745d944e4b78f2eb4dc1a1b92d08a37fe70f58fd15cbb73860\n",
      "  Stored in directory: /root/.cache/pip/wheels/ed/63/38/01208ae8de3d32f8ea6cda26e186e7e187bad3d105dd3b4d25\n",
      "  Building wheel for dsdk (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dsdk: filename=dsdk-1.4.7-py2.py3-none-any.whl size=23998 sha256=5c9cb0b241e830f84c5920b1397940d089480eeeff41321f4e1fb5b70207e0bb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d53y0d_a/wheels/e6/54/21/1cc5e6911ab312b107a853c3a6671c81274ea54a5f27564e5a\n",
      "Successfully built flake8-logging-format flake8-mutable dsdk\n",
      "Installing collected packages: pyflakes, pycodestyle, mccabe, wrapt, toml, snowballstemmer, py, pluggy, lazy-object-proxy, iniconfig, flake8, filelock, dsdk, distlib, coverage, virtualenv, pytest, pydocstyle, nodeenv, identify, flake8-polyfill, cfgv, astroid, types-pyyaml, types-python-dateutil, types-pkg-resources, pytest-cov, pylint, pre-commit, pep8-naming, mypy, flake8-sorted-keys, flake8-mutable, flake8-logging-format, flake8-docstrings, flake8-comprehensions, flake8-commas, flake8-bugbear, example\n",
      "  Attempting uninstall: dsdk\n",
      "    Found existing installation: dsdk 1.4.6\n",
      "    Uninstalling dsdk-1.4.6:\n",
      "      Successfully uninstalled dsdk-1.4.6\n",
      "  Running setup.py develop for example\n",
      "Successfully installed astroid-2.9.3 cfgv-3.3.1 coverage-6.2 distlib-0.3.4 dsdk-1.4.7 example-1.2.0rc4.dev3 filelock-3.4.2 flake8-4.0.1 flake8-bugbear-22.1.11 flake8-commas-2.1.0 flake8-comprehensions-3.8.0 flake8-docstrings-1.6.0 flake8-logging-format-0.6.0 flake8-mutable-1.2.0 flake8-polyfill-1.0.2 flake8-sorted-keys-0.2.0 identify-2.4.4 iniconfig-1.1.1 lazy-object-proxy-1.7.1 mccabe-0.6.1 mypy-0.931 nodeenv-1.6.0 pep8-naming-0.12.1 pluggy-1.0.0 pre-commit-2.16.0 py-1.11.0 pycodestyle-2.8.0 pydocstyle-6.1.1 pyflakes-2.4.0 pylint-2.12.2 pytest-6.2.5 pytest-cov-3.0.0 snowballstemmer-2.2.0 toml-0.10.2 types-pkg-resources-0.1.3 types-python-dateutil-2.8.6 types-pyyaml-6.0.3 virtualenv-20.13.0 wrapt-1.13.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c89f2e-1625-47a5-96c1-09cd820e46de",
   "metadata": {},
   "source": [
    "## 1.1.1 Setup (continued):\n",
    "\n",
    "3. Reload the conda env after each production mode install and once after a development mode install. Use the restart button next to the run and stop buttons in the toolbar.\n",
    "\n",
    "4. Check outdated packages for any unexpected suprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122a1de6-5b94-421e-955f-263a1ef9e3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version Latest Type\n",
      "------------------ ------- ------ -----\n",
      "anyio              3.4.0   3.5.0  wheel\n",
      "charset-normalizer 2.0.9   2.0.10 wheel\n",
      "GitPython          3.1.25  3.1.26 wheel\n",
      "idna               3.1     3.3    wheel\n",
      "ipykernel          6.6.1   6.7.0  wheel\n",
      "ipython            7.31.0  8.0.0  wheel\n",
      "json5              0.9.5   0.9.6  wheel\n",
      "jsonschema         4.3.3   4.4.0  wheel\n",
      "jupyter-server     1.13.1  1.13.2 wheel\n",
      "jupyterlab         3.2.5   3.2.8  wheel\n",
      "jupyterlab-git     0.34.0  0.34.1 wheel\n",
      "jupyterlab-server  2.10.2  2.10.3 wheel\n",
      "mistune            0.8.4   2.0.2  wheel\n",
      "nbclassic          0.3.4   0.3.5  wheel\n",
      "nbclient           0.5.9   0.5.10 wheel\n",
      "notebook           6.4.6   6.4.7  wheel\n",
      "numexpr            2.8.0   2.8.1  wheel\n",
      "Pillow             8.4.0   9.0.0  wheel\n",
      "platformdirs       2.3.0   2.4.1  wheel\n",
      "psycopg2           2.9.2   2.9.3  sdist\n",
      "Pygments           2.11.1  2.11.2 wheel\n",
      "PyQt5              5.12.3  5.15.6 wheel\n",
      "PyQt5_sip          4.19.18 12.9.0 wheel\n",
      "PyQtChart          5.12    5.15.5 wheel\n",
      "PyQtWebEngine      5.12.1  5.15.5 wheel\n",
      "requests           2.27.0  2.27.1 wheel\n",
      "retrolab           0.3.15  0.3.16 wheel\n",
      "smmap              3.0.5   5.0.0  wheel\n",
      "tables             3.6.1   3.7.0  wheel\n",
      "tomli              1.2.2   2.0.0  wheel\n",
      "urllib3            1.26.7  1.26.8 wheel\n",
      "zipp               3.6.0   3.7.0  wheel\n"
     ]
    }
   ],
   "source": [
    "!pip list --outdated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b20e27-4caf-4575-9e94-b9e655e16c14",
   "metadata": {},
   "source": [
    "## 2. Manage Configuration & Environment\n",
    "\n",
    "### 2.1. Files\n",
    "\n",
    "Because deserializing objects is less error prone than (re-)configuring previously existing python objects, use cfgenvy to load and dump yaml as configuration. Merge environment variable files into yaml configuration during deserialization, and keep your secrets separate and safe.\n",
    "\n",
    "Secrets as well as differences among deployment environments are placed in .env files: `./predict/secrets/`.\n",
    "\n",
    "Configurations are placed in .yaml files: `./predict/local/`.\n",
    "\n",
    "These directories have .gitignore protection from accidental inclusion in version control. See the `./predict/secrets/.gitignore` and `./predict/local/.gitignore` files for file names that *ARE* included in version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941def67-9670-40f5-a812-73faf06e5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "from dsdk import Asset, Mssql, Postgres\n",
    "from cfgenvy import yaml_loads, yaml_dumps, Parser, YamlMapping\n",
    "from typing import Any, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce0457-3243-455a-b92c-7cbfb0076068",
   "metadata": {},
   "source": [
    "Some file names and a parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212e8af7-bd38-43fa-9783-418747673989",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/tmp')\"\n",
    "config_file = \"./predict/local/notebook.example.yaml\"\n",
    "env_file = \"./predict/secrets/notebook.example.env\"\n",
    "\n",
    "parser = Parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedffd27-b17a-4ce9-98aa-97fe7fa19c11",
   "metadata": {},
   "source": [
    "Service names are resolved to host ip addresses by docker DNS as listed in docker-compose.override.yaml, and later by consul DNS in production. Use service names when possible instead of ip addresses. Even names for external services external like clarity, and epic can be registered in consul DNS to keep ip addresses out of configuration files.\n",
    "\n",
    "Here the MSSQL_HOST and POSTGRES_HOST are service names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a9f7dd-9860-4304-94b2-af4631888652",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs_str = \"\"\"\n",
    "EPIC_COOKIE=cookie\n",
    "MSSQL_DATABASE=clarity\n",
    "MSSQL_HOST=mssql\n",
    "MSSQL_PASSWORD=password\n",
    "MSSQL_PORT=1433\n",
    "MSSQL_USERNAME=username\n",
    "POSTGRES_DATABASE=test\n",
    "POSTGRES_HOST=postgres\n",
    "POSTGRES_PASSWORD=password\n",
    "POSTGRES_PORT=5432\n",
    "POSTGRES_SCHEMA=test\n",
    "POSTGRES_USERNAME=postgres\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8886cf-6a55-4003-a9d1-ce39197b194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(env_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.write(envs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01672cc-6c4b-47e2-b3fe-07bfa6f42bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs_str = \"\"\"\n",
    "elixhauser:\n",
    "  key1: val1\n",
    "  key2: val2\n",
    "  key3: val3\n",
    "mssql: !mssql\n",
    "  database: ${MSSQL_DATABASE}\n",
    "  host: ${MSSQL_HOST}\n",
    "  password: ${MSSQL_PASSWORD}\n",
    "  port: ${MSSQL_PORT}\n",
    "  schema: test\n",
    "  sql: !asset\n",
    "    path: ./predict/sql/mssql\n",
    "    ext: .sql\n",
    "  username: ${MSSQL_USERNAME}\n",
    "postgres: !postgres\n",
    "  database: ${POSTGRES_DATABASE}\n",
    "  host: ${POSTGRES_HOST}\n",
    "  password: ${POSTGRES_PASSWORD}\n",
    "  port: ${POSTGRES_PORT}\n",
    "  schema: test\n",
    "  sql: !asset\n",
    "    path: ./predict/sql/postgres\n",
    "    ext: .sql\n",
    "  username: ${POSTGRES_USERNAME}\n",
    "stages:\n",
    "- first\n",
    "- second\n",
    "- third\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b156e31-9102-4f17-8d7c-8f3f55330d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.write(cfgs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156c2a0-dac6-4aac-81fa-78d38047c366",
   "metadata": {},
   "source": [
    "Register classes as yaml types so they may be deserialized as instaces of python classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "722f9425-b04a-483c-9664-399c5f99fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(cfg): <class 'dict'>\n",
      "type(cfg['elixhauser']: <class 'dict'>\n",
      "type(cfg['postgres']: <class 'dsdk.postgres.Persistor'>\n",
      "type(cfg['postgres'].sql: <class 'dsdk.asset.Asset'>\n",
      "type(cfg['stages']): <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "Mssql.as_yaml_type()\n",
    "Postgres.as_yaml_type()\n",
    "\n",
    "cfg = parser.load(\n",
    "    config_file=config_file,\n",
    "    env_file=env_file,\n",
    ")\n",
    "\n",
    "print(f\"type(cfg): {type(cfg)}\")\n",
    "print(f\"type(cfg['elixhauser']: {type(cfg['elixhauser'])}\")\n",
    "print(f\"type(cfg['postgres']: {type(cfg['postgres'])}\")\n",
    "print(f\"type(cfg['postgres'].sql: {type(cfg['postgres'].sql)}\")\n",
    "print(f\"type(cfg['stages']): {type(cfg['stages'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d8a71-5133-44bf-a103-dfd23b68b0b0",
   "metadata": {},
   "source": [
    "Create and register a class to provide better validation for confguration and by ensuring that the configuration file is not mismatched, use explicit yaml `!<type>` and a clss. Unlike a python dictionary, unexpected or missing keywords will raise early exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acfd8b1c-d259-4213-b9f2-256297bb8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.write(\"!cfg\" + cfgs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87ee8d93-75bb-4c10-ad21-75a672c34d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cfg(YamlMapping):\n",
    "\n",
    "    YAML = '!cfg'\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        elixhauser: Dict[str, str],\n",
    "        mssql: Mssql,\n",
    "        postgres: Postgres,\n",
    "        stages: List,\n",
    "    ):\n",
    "        self.elixhauser = elixhauser\n",
    "        self.mssql = mssql\n",
    "        self.postgres = postgres\n",
    "        self.stages = stages\n",
    "    \n",
    "    def as_yaml(self) -> Dict[str, Any]:\n",
    "        \"\"\"As yaml.\"\"\"\n",
    "        return {\n",
    "            \"elixhauser\": self.elixhauser,\n",
    "            \"mssql\": self.mssql,\n",
    "            \"postgres\": self.postgres,\n",
    "            \"stages\": self.stages,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ba1b4e-83e4-4438-a825-5e2a10f29ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(cfg): <class '__main__.Cfg'>\n",
      "type(cfg.elixhauser): <class 'dict'>\n",
      "type(cfg.postgres): <class 'dsdk.postgres.Persistor'>\n",
      "type(cfg.postgres.sql): <class 'dsdk.asset.Asset'>\n",
      "type(cfg.stages): <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "Cfg.as_yaml_type()\n",
    "\n",
    "cfg = parser.load(\n",
    "    config_file=config_file,\n",
    "    env_file=env_file,\n",
    ")\n",
    "\n",
    "print(f\"type(cfg): {type(cfg)}\")\n",
    "print(f\"type(cfg.elixhauser): {type(cfg.elixhauser)}\")\n",
    "print(f\"type(cfg.postgres): {type(cfg.postgres)}\")\n",
    "print(f\"type(cfg.postgres.sql): {type(cfg.postgres.sql)}\")\n",
    "print(f\"type(cfg.stages): {type(cfg.stages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d232f7ed-66aa-4088-88a3-92824bdcf0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres = cfg.postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13403b07-2b21-4a5a-9f9b-4ba2313cd98a",
   "metadata": {},
   "source": [
    "Debug the final merged configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57cb790a-42a9-49c1-a29b-36534106adc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!postgres\n",
      "database: test\n",
      "host: postgres\n",
      "password: password\n",
      "port: '5432'\n",
      "schema: test\n",
      "sql: !asset\n",
      "  ext: .sql\n",
      "  path: ./predict/sql/postgres\n",
      "username: postgres\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(yaml_dumps(postgres))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883cdb6f-32ae-4d10-8ec2-10af7512f8af",
   "metadata": {},
   "source": [
    "## 3. Check Database Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af65f71-0588-40cb-89bb-0cd7710ef5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Very database, much wow!',)\n"
     ]
    }
   ],
   "source": [
    "with postgres.rollback() as cursor:\n",
    "    cursor.execute(\"\"\"select 'Very database, much wow!' as doge\"\"\")\n",
    "    rows = cursor.fetchall()\n",
    "    print(rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1cf36",
   "metadata": {},
   "source": [
    "## 4. Manage SQL & Other Text Assets\n",
    "\n",
    "Assets loads text files from disk. Unlike SQL embedded in python strings, SQL syntax highlighting may be available in text editor. The python placeholders expected by psycopg2 and pymssql will still be marked as errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7a7bc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select\n",
      "    score\n",
      "from\n",
      "    predictions\n",
      "where\n",
      "    run_id = %(run_id)s\n",
      "order by\n",
      "    id desc;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(postgres.sql.predictions.gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0faa1ce-1357-4a3e-bdd2-d9298fc153a8",
   "metadata": {},
   "source": [
    "# 5. Rethink SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e91d456b-5405-4d64-8ddd-11c40a03a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {\n",
    "    \"cohort\": ('00001', '00002', '00003'),\n",
    "    \"conditions\" : ('sleepy', 'happy', 'grumpy'),\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    \"dry_run\": 0,\n",
    "    \"cohort_begin\": '2021-05-05',\n",
    "    \"cohort_end\": '2021-05-06',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f508827",
   "metadata": {},
   "source": [
    "## 5.1. Prefer `with` over `in (?, ...)`:\n",
    "\n",
    "Avoid `in` for more than a few elements:\n",
    "\n",
    "`select * from patients where id in ('00001', '00002', '00003', ...);`\n",
    "\n",
    "Unfortunately, the execution plan renders `in` similar to multiple `or`:\n",
    "\n",
    "`select * from patients where id = '00001' or id = '00002' or id = '00003' ...;`\n",
    "\n",
    "The performance is terrible. The database has limits on the number of elements that may be included using `in (?, ...)`. Fundamentally, the database does not treat `in` like a table with a single column, in part because the column data type is not known. Client languages like python typically only have data types that approximately match the database's data types. For example the pymssql driver passes all python strings to mssql as `nvarchar` literals ('n' is not a typo). Each element is coherced to the most permissive data type during comparison. This implicit, permissive casting and cohersion prevents indices from being used.\n",
    "\n",
    "Use `with` instead and `cast` the column to the appropriate data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748747ae",
   "metadata": {},
   "source": [
    "### 5.1.1. Example:\n",
    "\n",
    "An easy example in templated sql for python and dsdk looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9193176b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id\n",
       "0  00001\n",
       "1  00002\n",
       "2  00003"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_5_1_1 = '''\n",
    "with cohort as (\n",
    "    select cast(null as varchar(8)) as id -- data type is on the cohort.id column, not just this first row\n",
    "    {cohort}\n",
    ")\n",
    "select\n",
    "    id\n",
    "from\n",
    "    cohort\n",
    "where\n",
    "    id is not null;'''\n",
    "\n",
    "with postgres.rollback() as cur:\n",
    "    df = postgres.df_from_query_by_keys(cur, query_5_1_1, keys, parameters)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0035b",
   "metadata": {},
   "source": [
    "### 5.1.2. Example:\n",
    "\n",
    "A more useful example using dsdk looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68b1c73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_begin</th>\n",
       "      <th>cohort_end</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00001</td>\n",
       "      <td>sleepy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00001</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00001</td>\n",
       "      <td>grumpy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00002</td>\n",
       "      <td>sleepy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00002</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00002</td>\n",
       "      <td>grumpy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00003</td>\n",
       "      <td>sleepy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00003</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00003</td>\n",
       "      <td>grumpy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               cohort_begin                cohort_end     id    name\n",
       "0 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00001  sleepy\n",
       "1 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00001   happy\n",
       "2 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00001  grumpy\n",
       "3 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00002  sleepy\n",
       "4 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00002   happy\n",
       "5 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00002  grumpy\n",
       "6 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00003  sleepy\n",
       "7 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00003   happy\n",
       "8 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00003  grumpy"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_5_1_2 = '''\n",
    "with args as (\n",
    "    select\n",
    "        cast(%(cohort_begin)s as timestamptz) as cohort_begin,\n",
    "        cast(%(cohort_end)s as timestamptz) as cohort_end\n",
    "), cohort as (\n",
    "    select cast(null as varchar(8)) as id\n",
    "    {cohort}\n",
    "), conditions as (\n",
    "    select cast(null as varchar(16)) as name\n",
    "    {conditions}\n",
    ")\n",
    "select\n",
    "    cohort_begin,\n",
    "    cohort_end,\n",
    "    id,\n",
    "    name\n",
    "from\n",
    "    args\n",
    "    join cohort\n",
    "        on id is not null\n",
    "    join conditions\n",
    "        on name is not null;\n",
    "'''\n",
    "\n",
    "with postgres.rollback() as cur:\n",
    "    df = postgres.df_from_query_by_keys(cur, query_5_1_2, keys, parameters)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1152c19",
   "metadata": {},
   "source": [
    "### 5.1.3. Example\n",
    "\n",
    "Implementation of dsdk for df_from_query_by_keys uses `union all select` implementation. This formulation avoids item limits as well as comma counting of `insert (...) values (...), ...`. Unlike `in` and `insert (...) values (...), ...` it also results in perfectly valid sql even when the cohort or conditions lists empty, because the empty lists render as code while retaining the column data type(s) using the \"null row\".\n",
    "\n",
    "Unwind the sequences and replace the placeholders in pgadmin, DBeaver, Data Grip, and Microsort Sql Server Management Studio to test and explain your queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d4bb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_begin</th>\n",
       "      <th>cohort_end</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00001</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00001</td>\n",
       "      <td>sleepy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00001</td>\n",
       "      <td>grumpy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00002</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00002</td>\n",
       "      <td>sleepy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00002</td>\n",
       "      <td>grumpy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00003</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00003</td>\n",
       "      <td>sleepy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-05-05 00:00:00+00:00</td>\n",
       "      <td>2021-05-06 00:00:00+00:00</td>\n",
       "      <td>00003</td>\n",
       "      <td>grumpy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               cohort_begin                cohort_end     id    name\n",
       "0 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00001   happy\n",
       "1 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00001  sleepy\n",
       "2 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00001  grumpy\n",
       "3 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00002   happy\n",
       "4 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00002  sleepy\n",
       "5 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00002  grumpy\n",
       "6 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00003   happy\n",
       "7 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00003  sleepy\n",
       "8 2021-05-05 00:00:00+00:00 2021-05-06 00:00:00+00:00  00003  grumpy"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_5_1_3 = '''\n",
    "with args as (\n",
    "    select\n",
    "        cast('2021-05-05' as timestamptz) as cohort_begin,\n",
    "        cast('2021-05-06' as timestamptz) as cohort_end\n",
    "), cohort as (\n",
    "    select cast(null as varchar) as id\n",
    "    union all select '00001'\n",
    "    union all select '00002'\n",
    "    union all select '00003'\n",
    "), conditions as (\n",
    "    select cast(null as varchar) as name\n",
    "    union all select 'happy'\n",
    "    union all select 'sleepy'\n",
    "    union all select 'grumpy'\n",
    ")\n",
    "select\n",
    "    cohort_begin,\n",
    "    cohort_end,\n",
    "    id,\n",
    "    name\n",
    "from\n",
    "    args\n",
    "    join cohort\n",
    "        on id is not null\n",
    "    join conditions\n",
    "        on name is not null;'''\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "with postgres.rollback() as cur:\n",
    "    cur.execute(query_5_1_3)\n",
    "    rows = cur.fetchall()\n",
    "    df = DataFrame(rows)\n",
    "    columns = (each[0] for each in cur.description)\n",
    "    df.columns = columns\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561cec34-f4f5-4c93-a320-50c97a2fed12",
   "metadata": {},
   "source": [
    "## 5.2 Use dry run to fail early\n",
    "\n",
    "Make the database do more work for you. This includes validating some syntax and all permission on the service accounts BEFORE passing actual useful data to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e880100f-f0e7-44f4-b4b9-d367e4209ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5_2 = '''\n",
    "with vars as (\n",
    "    select\n",
    "        cast(coalesce(%(dry_run)s, 1) as int) as dry_run,\n",
    "        cast(%(cohort_begin)s as timestamptz) as cohort_begin,\n",
    "        cast(%(cohort_end)s as timestamptz) as cohort_end\n",
    "), cohort as (\n",
    "    select cast(null as varchar) as id\n",
    "    {cohort}\n",
    ")\n",
    "select\n",
    "    no_such_table.*\n",
    "from\n",
    "    vars as v\n",
    "    join cohort as c\n",
    "        on v.dry_run = 0\n",
    "        and c.id is not null;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9ca12f2-278a-41b6-b6ad-209d3b26de78",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedTable",
     "evalue": "missing FROM-clause entry for table \"no_such_table\"\nLINE 12:     no_such_table.*\n             ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedTable\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/2484022709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpostgres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdry_run_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_5_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.venv/lib/python3.9/site-packages/dsdk/persistor.py\u001b[0m in \u001b[0;36mdry_run_query\u001b[0;34m(self, query, parameters)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".sql\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrendered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrendered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUndefinedTable\u001b[0m: missing FROM-clause entry for table \"no_such_table\"\nLINE 12:     no_such_table.*\n             ^\n"
     ]
    }
   ],
   "source": [
    "postgres.dry_run_query(query_5_2, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ff7ac-d95a-42d7-aad8-e14b6f58e0a8",
   "metadata": {},
   "source": [
    "Persistors can dry run all sql queries in an asset if all parameters are provided. All queries must be written to select, insert, update or delete no data when dry_run is 1, but must do by producing empty data sets for insert, update, and delete instead of exiting early.\n",
    "\n",
    "Typically, this means using a `with` clause to build a data set for insert, update or delete and performing a join on `dry_run = 0` that knocks out all rows from the data manipulation operators.\n",
    "\n",
    "More examples to come, and all queries in the postgres persistor asset must be revised for dry_run compatibility.\n",
    "\n",
    "More examples to come on when to add unused tables to aquire indices.\n",
    "\n",
    "More examples to come on sql performance profiling and explain.\n",
    "\n",
    "More example on when using temp tables may be an advantage, and the impact on readability, maintainability, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e37c8f-540c-42a6-99b7-d9091e27994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres.dry_run({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed9239-bbc9-4aa6-87c5-e462db7d925a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.venv] *",
   "language": "python",
   "name": "conda-env-.venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
